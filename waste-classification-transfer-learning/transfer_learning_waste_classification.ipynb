{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required Python packages with specific versions\n",
    "!pip install tensorflow==2.17.0 | tail -n 1\n",
    "!pip install numpy==1.26.0 | tail -n 1\n",
    "!pip install scikit-learn==1.5.1  | tail -n 1\n",
    "!pip install matplotlib==3.9.2  | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.image import imread\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn import metrics\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract dataset with progress indication\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/kd6057VPpABQ2FqCbgu9YQ/o-vs-r-split-reduced-1200.zip\"\n",
    "file_name = \"o-vs-r-split-reduced-1200.zip\"\n",
    "\n",
    "print(\"Downloading file\")\n",
    "with requests.get(url, stream=True) as response:\n",
    "    response.raise_for_status()\n",
    "    with open(file_name, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "\n",
    "def extract_file_with_progress(file_name):\n",
    "    print(\"Extracting file with progress\")\n",
    "    with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "        members = zip_ref.infolist() \n",
    "        with tqdm(total=len(members), unit='file') as progress_bar:\n",
    "            for member in members:\n",
    "                zip_ref.extract(member)\n",
    "                progress_bar.update(1)\n",
    "    print(\"Finished extracting file\")\n",
    "\n",
    "\n",
    "extract_file_with_progress(file_name)\n",
    "\n",
    "print(\"Finished extracting file\")\n",
    "os.remove(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables\n",
    "img_rows, img_cols = 150, 150\n",
    "batch_size = 32\n",
    "n_epochs = 10\n",
    "n_classes = 2\n",
    "val_split = 0.2\n",
    "verbosity = 1\n",
    "path = 'o-vs-r-split/train/'\n",
    "path_test = 'o-vs-r-split/test/'\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "labels = ['O', 'R']\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load test images and labels into numpy arrays\n",
    "\n",
    "# Create ImageDataGenerators for training and validation and testing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    validation_split = val_split,\n",
    "    rescale=1.0/255.0,\n",
    "\twidth_shift_range=0.1, \n",
    "    height_shift_range=0.1, \n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    validation_split = val_split,\n",
    "    rescale=1.0/255.0,\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training data generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory = path,\n",
    "    seed = seed,\n",
    "    batch_size = batch_size, \n",
    "    class_mode='binary',\n",
    "    shuffle = True,\n",
    "    target_size=(img_rows, img_cols),\n",
    "    subset = 'training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the validation data generator\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    directory = path,\n",
    "    seed = seed,\n",
    "    batch_size = batch_size, \n",
    "    class_mode='binary',\n",
    "    shuffle = True,\n",
    "    target_size=(img_rows, img_cols),\n",
    "    subset = 'validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the testing data generator\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory = path_test,\n",
    "    class_mode = 'binary',\n",
    "    seed = seed,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    target_size = (img_rows, img_cols),\n",
    ")\n",
    "\n",
    "print(len(train_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize augmented images\n",
    "from pathlib import Path\n",
    "\n",
    "IMG_DIM = (100, 100)\n",
    "\n",
    "train_files = glob.glob('./o-vs-r-split/train/O/*')\n",
    "train_files = train_files[:20]\n",
    "train_imgs = [tf.keras.preprocessing.image.img_to_array(tf.keras.preprocessing.image.load_img(img, target_size=IMG_DIM)) for img in train_files]\n",
    "train_imgs = np.array(train_imgs)\n",
    "train_labels = [Path(fn).parent.name for fn in train_files]\n",
    "\n",
    "img_id = 0\n",
    "O_generator = train_datagen.flow(train_imgs[img_id:img_id+1], train_labels[img_id:img_id+1],\n",
    "                                   batch_size=1)\n",
    "O = [next(O_generator) for i in range(0,5)]\n",
    "fig, ax = plt.subplots(1,5, figsize=(16, 6))\n",
    "print('Labels:', [item[1][0] for item in O])\n",
    "l = [ax[i].imshow(O[i][0][0]) for i in range(0,5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 model without the top classification layers\n",
    "from tensorflow.keras.applications import vgg16\n",
    "\n",
    "input_shape = (150, 150, 3)\n",
    "vgg = vgg16.VGG16(include_top=False,\n",
    "                        weights='imagenet',\n",
    "                        input_shape=input_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model that outputs the flattened features from VGG16\n",
    "output = vgg.layers[-1].output\n",
    "output = tf.keras.layers.Flatten()(output)\n",
    "basemodel = Model(vgg.input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the layers of the base model\n",
    "for layer in basemodel.layers: \n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the final model by adding custom layers on top of the base model\n",
    "input_shape = basemodel.output_shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(basemodel)\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile the model\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in basemodel.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = optimizers.RMSprop(learning_rate=1e-4),\n",
    "    metrics = ['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks including early stopping and model checkpointing\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "checkpoint_path='O_R_tlearn_vgg16.keras'\n",
    "\n",
    "# define step decay function\n",
    "class LossHistory_(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.lr = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.lr.append(exp_decay(epoch))\n",
    "        print('lr:', exp_decay(len(self.losses)))\n",
    "\n",
    "def exp_decay(epoch):\n",
    "    initial_lrate = 1e-4\n",
    "    k = 0.1\n",
    "    lrate = initial_lrate * np.exp(-k*epoch)\n",
    "    return lrate\n",
    "\n",
    "# learning schedule callback\n",
    "loss_history_ = LossHistory_()\n",
    "lrate_ = LearningRateScheduler(exp_decay)\n",
    "\n",
    "keras_callbacks = [\n",
    "      EarlyStopping(monitor = 'val_loss', \n",
    "                    patience = 4, \n",
    "                    mode = 'min', \n",
    "                    min_delta=0.01),\n",
    "      ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, mode='min')\n",
    "]\n",
    "\n",
    "callbacks_list_ = [loss_history_, lrate_] + keras_callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "extract_feat_model = model.fit(train_generator, \n",
    "                               steps_per_epoch=5, \n",
    "                               epochs=10,\n",
    "                               callbacks = callbacks_list_,   \n",
    "                               validation_data=val_generator, \n",
    "                               validation_steps=val_generator.samples // batch_size, \n",
    "                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = extract_feat_model\n",
    "\n",
    "# plot loss curve\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy curves for training and validation sets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = extract_feat_model\n",
    "\n",
    "## Plot accuracy curves for training and validation sets\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model by unfreezing some layers of the base model\n",
    "from tensorflow.keras.applications import vgg16\n",
    "\n",
    "input_shape = (150, 150, 3)\n",
    "vgg = vgg16.VGG16(include_top=False,\n",
    "                        weights='imagenet',\n",
    "                        input_shape=input_shape)\n",
    "\n",
    "output = vgg.layers[-1].output\n",
    "output = tf.keras.layers.Flatten()(output)\n",
    "basemodel = Model(vgg.input, output)\n",
    "\n",
    "for layer in basemodel.layers: \n",
    "    layer.trainable = False\n",
    "\n",
    "display([layer.name for layer in basemodel.layers])\n",
    "\n",
    "set_trainable = False\n",
    "\n",
    "for layer in basemodel.layers:\n",
    "    if layer.name in ['block5_conv3']:\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "for layer in basemodel.layers:\n",
    "    print(f\"{layer.name}: {layer.trainable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the final model by adding custom layers on top of the base model and add dropout layer\n",
    "\n",
    "model = Sequential()\n",
    "model.add(basemodel)\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "checkpoint_path='O_R_tlearn_fine_tune_vgg16.keras'\n",
    "\n",
    "# learning schedule callback\n",
    "loss_history_ = LossHistory_()\n",
    "lrate_ = LearningRateScheduler(exp_decay)\n",
    "\n",
    "keras_callbacks = [\n",
    "      EarlyStopping(monitor = 'val_loss', \n",
    "                    patience = 4, \n",
    "                    mode = 'min', \n",
    "                    min_delta=0.01),\n",
    "      ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, mode='min')\n",
    "]\n",
    "\n",
    "callbacks_list_ = [loss_history_, lrate_] + keras_callbacks\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "fine_tune_model = model.fit(train_generator, \n",
    "                    steps_per_epoch=5, \n",
    "                    epochs=10,\n",
    "                    callbacks = callbacks_list_,   \n",
    "                    validation_data=val_generator, \n",
    "                    validation_steps=val_generator.samples // batch_size, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves for training and validation sets (fine tune model)\n",
    "\n",
    "history = fine_tune_model\n",
    "\n",
    "## Plot loss curves for training and validation sets (fine tune model)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy curves for training and validation sets (fine tune model)\n",
    "history = fine_tune_model\n",
    "\n",
    "# Task 8: Plot accuracy curves for training and validation sets  (fine tune model)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models on the test set and print classification reports\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Load saved models\n",
    "extract_feat_model = tf.keras.models.load_model('O_R_tlearn_vgg16.keras')\n",
    "fine_tune_model = tf.keras.models.load_model('O_R_tlearn_fine_tune_vgg16.keras')\n",
    "\n",
    "IMG_DIM = (150, 150)\n",
    "\n",
    "# Load test images\n",
    "test_files_O = glob.glob('./o-vs-r-split/test/O/*')\n",
    "test_files_R = glob.glob('./o-vs-r-split/test/R/*')\n",
    "test_files = test_files_O[:50] + test_files_R[:50]\n",
    "\n",
    "test_imgs = [tf.keras.preprocessing.image.img_to_array(tf.keras.preprocessing.image.load_img(img, target_size=IMG_DIM)) for img in test_files]\n",
    "test_imgs = np.array(test_imgs)\n",
    "test_labels = [Path(fn).parent.name for fn in test_files]\n",
    "\n",
    "# Standardize\n",
    "test_imgs_scaled = test_imgs.astype('float32')\n",
    "test_imgs_scaled /= 255\n",
    "\n",
    "class2num_lt = lambda l: [0 if x == 'O' else 1 for x in l]\n",
    "num2class_lt = lambda l: ['O' if x < 0.5 else 'R' for x in l]\n",
    "\n",
    "test_labels_enc = class2num_lt(test_labels)\n",
    "\n",
    "# Make predictions for both models\n",
    "predictions_extract_feat_model = extract_feat_model.predict(test_imgs_scaled, verbose=0)\n",
    "predictions_fine_tune_model = fine_tune_model.predict(test_imgs_scaled, verbose=0)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predictions_extract_feat_model = num2class_lt(predictions_extract_feat_model)\n",
    "predictions_fine_tune_model = num2class_lt(predictions_fine_tune_model)\n",
    "\n",
    "# Print classification report for both models\n",
    "print('Extract Features Model')\n",
    "print(metrics.classification_report(test_labels, predictions_extract_feat_model))\n",
    "print('Fine-Tuned Model')\n",
    "print(metrics.classification_report(test_labels, predictions_fine_tune_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one of the images with actual label and predicted label as title\n",
    "def plot_image_with_title(image, model_name, actual_label, predicted_label):\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Model: {model_name}, Actual: {actual_label}, Predicted: {predicted_label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Specify index of image to plot, for example index 0\n",
    "index_to_plot = 0\n",
    "plot_image_with_title(\n",
    "    image=test_imgs[index_to_plot].astype('uint8'),\n",
    "    model_name='Extract Features Model',\n",
    "    actual_label=test_labels[index_to_plot], \n",
    "    predicted_label=predictions_extract_feat_model[index_to_plot],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a test image using Extract Features Model\n",
    "index_to_plot = 1\n",
    "plot_image_with_title(\n",
    "    image=test_imgs[index_to_plot].astype('uint8'),\n",
    "    model_name='Extract Features Model',\n",
    "    actual_label=test_labels[index_to_plot],\n",
    "    predicted_label=predictions_extract_feat_model[index_to_plot],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a test image using Fine-Tuned Model\n",
    "index_to_plot = 1\n",
    "plot_image_with_title(\n",
    "    image=test_imgs[index_to_plot].astype('uint8'),\n",
    "    model_name='Fine-Tuned Model',\n",
    "    actual_label=test_labels[index_to_plot],\n",
    "    predicted_label=predictions_fine_tune_model[index_to_plot],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
